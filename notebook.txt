Sunday 29th October

I've decided not to implement a complete solution for timestamps. For now, I'll keep the flat structure of phrases
and timestamp events and most consequences with current time. The exception is knowledge, which will get the timestamp
of the earliest phrase in the phrase list.

In practice, whenever we add consequences, whether for events or static knowledge, we give the timestamp of the record, the current
record.

As of now, there is no inferring from the knowledge base of a specific person. So even if John knows that Daryl went to Fiji,
he does not directly infer that Daryl is in Fiji. Instead we use inference from the static db (objective knowledge) to do that

Probably would make more sense to have a centralized step creation mechanism that includes events, inter-person questions, responses etc.
Attach a gens-type, such as stroy-step, knowledge insertion, etc.

Tuesday 31st October

Pushed the commit 'Answering personal questions'

An answer is generated but the next step is to add the contents of the answer as knowledge

Wednesday 1st November

Finished adding the answer to an inter-person query into the story database.

Will commit a version with above string

I intend to remove the generation of phrases for rules just for the purpose of learning because (1) that is about to change
and (2) some rules such as the knowledge resulting from telling rule generated millions of records.

A thought about time:
4 kinds of phrases:
state (as in John wants the wheel - that is always true)
current (as in Mike has the purse - this automatically updates its time. If it was true last stem, unless removed,
        it is true the next step. Note true for objective facts but not knowledge)
event (Harry went to the UK. Always has a specific time)
phase (Terry knows that Mile has the purse - this is true for a time. It can go out of date unless specifically updated)

Further note. Currently knows that has a time stamp of the object of the knowledge. We may need to break down a phrase into
constinuent pieces wach with their own time.


It doesn't make sense to have so many functions, each creating rules. There should only be one function for rule but many
differnt types that use something like the mostly defunct 'story_based, b_db, b_story'. Each step in the story advnacing process
must pick out the kind of rules it needs and advance them.

Next, I will add motivations. The current idea is:
(1) A steady state of wanting
(2) If you want x and you know that x is located in y you go to y
(3) If you want x and you are located with x and x is free pick up x
If you want x and you are located with x and z has x ask z for x

Thursday 2nd Nov

Rearrangin rules. Removed defunct b_db and b_story that weren't being used anyway. Added rule_type that falls into a number of categories:
story_start - used to initialize where people and objects are and what objects people want
event_from_none - used to create a story step (event) based on other states (rather that events responding to events)
state_from_state - update states based on other states. For example if Jon is in UK and glass is in UK then Jon knows glass is in UK
state_from_event - update states as a result of an event. Jon has the purse and Jon went to France, so create a story phrase that the purse is in France (remove old)
event_from_event - used to create a story step (event) based on another event (John gives if John is asked)
block_event - used to block events. So a rule that says that Jon gave to Jon is blocked
knowledge_query  - used in response to queries to first create a personal db of the knowledge of a particular person
query - used to test any knowledge base

used the opportunity to make rule indentations consistent

we need to replace var_id numbers by symbolic constants. A next step could be to translate this to he current form. It's
    just very difficult to write rles without bugs using the current form

While I'm at it, do the same for referring to clauses within the AND. It will be useful for time stamps

Added a probability for event_from_none rules so that we can prioiritize debugging or events that are caused by wants

Added a probability for state_from_event so that some rules don't automatically run after an event (e.g. like)

The are only two like states: likes and nothing.
Giving something to somebody that they want is definately gonna cause a like
Telling somebody something, which can only come after a question might get a like

Committing with the label:
Added likes and instory queries

Monday 6th Nov

Committed version with the message:

"Preparing for learn. Added controlled garadual learning. curriculum. and generating possible rules to genrate results."

Added curriculum.py. Similar to story.py but gets specific rules by names. At first is just adds a simple pickup rule and
gets the 'has' consequence phrase.

This is then passed with the story and the event to cascade.py which finds all phrases that includes the input words and
then cascades on all the words it finds

Back in curruculum.py, take all the permutations of the cascaded phrases and for each perm creates a rule with no vars.
Next, grenerate a rule replacing all repeated occurences of words with vars and looking like a rule.

Now I will work to incorporate standard GloVe or Word2Vec vectors for all words so that we can create an embedding for the rule.

Wednesday 8th Nov

About to commit a version with the message:

Added parse.py and embed.py together to create .glv files for embedding.

Added two stand-alones. parse.py calls StanfordNLP to create a dep tree of each of the objects and actions.
It creates a .nlp file where each row contains
1. The original text
2. The number of words
3. For each word:
    The dep relation for which that word is the dependent
    The idx (1-based) of its governing word
    The word itself

embed.py creates a .glv for each of actions, names, objects and countries. Countries and names are simply looked up in
a GloVe database and the 50 values appended as an embedding. Objects and Names parse the .nlp and add the following
embed vectors:
1. the word itself - the vec from GloVe
2. The dep relation - a one hot binary float vec
3. The governing idx a one hot up to the max idx

If phrases are longer than 4 words they are rejected. If they are less they are padded with an <invalid> dep rel, zeros and
max index + 1.

fixed bug in els.py: make_vec. If there was only one record, the code output a shape of dim 1 instead of 2

Sunday 12th November

Start of day todo:

Move the initialization of elements into cuuriculum.py

Apply state to story and move the story along to enrich the types of rules we are trying to learn

Batch inputs of rules according to length of input

Put headers on the vec representation of phrases or rules. These headers recurse into the body

A header consists of a number and that number of offsets
    Any number can be represented as a 0-1 float by appliying a factor of ten multiplier. To make up to 1000, div by 10000
    Three els in a phrase (john, picked up, the ball) are represented as 3, 0, 125, 325 if that is where each el starts
    An el itself can be composed of deps, so an el begins with a header with the number of deps and their offsets
    A dep consists of the word, its dep rel and the index of the governor. So a dep, too begins with num (3) and offset

Monday 13th November

Yesterday I completed batching of inputs. For now I assume that in curriculum.py processes rules that come from cascade and
only have the sel_el field set and no groups. Therefore each rule can only generate one rec. Once the rules are batched by input
length, there needs to be an indexing into the original rule list or a rearrangement of that list.

After much thinking I think the best plan is as follows:

1. Learn a transformation of the rule vec into a short fixed-length key vector *key1*
    a. This transformation can consist of multiple nets, one for each length
    b. A net could be as simple as a matmul
2. Cascade to find all phrases related to an input event plus the event itself
3. Create all orderings of each component of the cascade (query options)
    a. Apply var_dict processing to each query option to make later matching easier
4. Transform the vec of all the query options to a key of the same length as 1.
5. Match each query option (not the vec) to each of the rules in the db. Generate the gens from the preconds *key2*
    a. The instantiation of the vars in the gens is from the values in the query option
    b. Padding is required so that all vars can get something - even if it is only an empty string
    c. Another option is to immediately decide that this query option fails to match on this rule
6. Compare the gens of the match in the last step to the training result of the event
    a. [TBD] what if there are more than one result
    b. If the (instantiated) gens matches, we want key1 and key2 to be close to each other (cosine distance close) to 1
        else, far apart

Committing before starting to implement this with the message:
Implemented batching of rules by vec length

